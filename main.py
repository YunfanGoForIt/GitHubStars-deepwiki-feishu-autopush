import os
import asyncio
import logging
import uvicorn
import datetime
from fastapi import FastAPI, Request, Depends
from fastapi.templating import Jinja2Templates
from sqlalchemy.orm import Session
from contextlib import asynccontextmanager
import glob
from enum import Enum
from typing import Dict, Any, Optional

from models import SessionLocal, init_db, ProcessedRepo, ProcessingStatus
from github_client import GitHubMonitor
from feishu_client import FeishuService
from mcp_client import DeepWikiMCPClient
from rag_refine import RAGRefiner, Config
import httpx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("mcp_deepwiki.log"),
        logging.StreamHandler()
    ],
    force=True  # override any previous logging.basicConfig (e.g., from imports)
)
logger = logging.getLogger(__name__)


class TaskType(Enum):
    SYNC = "sync"
    REGENERATE = "regenerate"


class TaskQueue:
    """ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨ - å•ä»“åº“ç²’åº¦"""
    def __init__(self):
        self.queue = asyncio.Queue()
        self.is_processing = False
        self.current_task = None
        self.task_list = []  # ä¿å­˜æ‰€æœ‰ä»»åŠ¡çš„åˆ—è¡¨ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        self.repo_ids_in_queue = set()  # è®°å½•é˜Ÿåˆ—ä¸­çš„ä»“åº“IDï¼Œç”¨äºå»é‡

    async def add_repo_task(self, task_type: TaskType, repo_data: Dict[str, Any]):
        """æ·»åŠ å•ä¸ªä»“åº“ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼šGitHub API æ ¼å¼ç”¨ "id"ï¼Œæ•°æ®åº“æ ¼å¼ç”¨ "repo_id"
        repo_id = repo_data.get("repo_id") or str(repo_data.get("id", ""))

        # æ£€æŸ¥æ˜¯å¦å·²åœ¨é˜Ÿåˆ—ä¸­
        if repo_id in self.repo_ids_in_queue:
            return False

        task = {
            "type": task_type,
            "data": repo_data,
            "added_at": datetime.datetime.now()
        }

        await self.queue.put(task)
        self.task_list.append(task)
        self.repo_ids_in_queue.add(repo_id)

        # å®šæœŸæ¸…ç†
        self._cleanup_completed_tasks()

        # è·å–ä»“åº“åç§°ç”¨äºæ—¥å¿—æ˜¾ç¤º
        repo_name = repo_data.get("full_name") or repo_data.get("repo_name", "Unknown")
        queue_size = self.queue.qsize()
        logger.info(f"ğŸ“¥ ä»“åº“å·²åŠ å…¥é˜Ÿåˆ—: {repo_name} (é˜Ÿåˆ—é•¿åº¦: {queue_size})")
        return True

    def is_repo_in_queue(self, repo_id: str) -> bool:
        """æ£€æŸ¥ä»“åº“æ˜¯å¦å·²åœ¨é˜Ÿåˆ—ä¸­"""
        return repo_id in self.repo_ids_in_queue

    def _cleanup_completed_tasks(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡ï¼ˆä¿ç•™æœ€è¿‘100ä¸ªï¼‰"""
        if len(self.task_list) > 100:
            self.task_list = self.task_list[-100:]

    async def get_next_task(self):
        """è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        task = await self.queue.get()
        self.current_task = task
        return task

    def mark_task_done(self):
        """æ ‡è®°å½“å‰ä»»åŠ¡å®Œæˆ"""
        if self.current_task:
            repo_id = self.current_task["data"].get("repo_id")
            if repo_id:
                self.repo_ids_in_queue.discard(repo_id)

            # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            self.task_list = [t for t in self.task_list if t != self.current_task]
            self.current_task = None
            self.queue.task_done()

    def get_waiting_tasks(self):
        """è·å–ç­‰å¾…ä¸­çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆä¸ä»é˜Ÿåˆ—ä¸­ç§»é™¤ï¼‰"""
        # è¿”å› task_list ä¸­è¿˜åœ¨é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
        return [t for t in self.task_list if t != self.current_task]

    def clear(self):
        """æ¸…ç©ºé˜Ÿåˆ—"""
        while not self.queue.empty():
            self.queue.get_nowait()
        self.task_list = []
        self.repo_ids_in_queue = set()
        logger.info("ğŸ—‘ï¸ ä»»åŠ¡é˜Ÿåˆ—å·²æ¸…ç©º")


# å…¨å±€ä»»åŠ¡é˜Ÿåˆ—
task_queue = TaskQueue()

# Initialize DB
init_db()

# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Service Instances
github_monitor = None
feishu_service = None
rag_refiner = None
mcp_client = None
deepwiki_indexer = None
templates = Jinja2Templates(directory="/www/wwwroot/mcp_deepwiki/templates")

config = Config()

async def process_repo_workflow(db: Session, repo_data: dict):
    repo_id = str(repo_data["id"])
    repo_name = repo_data["full_name"]
    github_url = repo_data["html_url"]
    
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()

    # Skip COMPLETED, PROCESSING, and SKIPPED repos, but allow retrying FAILED and PENDING repos
    if db_repo and (db_repo.status == ProcessingStatus.COMPLETED or db_repo.status == ProcessingStatus.PROCESSING or db_repo.status == ProcessingStatus.SKIPPED):
        return
        
    if not db_repo:
        db_repo = ProcessedRepo(
            repo_id=repo_id,
            repo_name=repo_name,
            repo_url=github_url,
            description=repo_data.get("description"),
            status=ProcessingStatus.PROCESSING
        )
        db.add(db_repo)
    else:
        db_repo.status = ProcessingStatus.PROCESSING
    db.commit()
    db.refresh(db_repo)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ä»“åº“: {repo_name}")
        
        safe_name = repo_name.replace("/", "_")
        base_dir = f"/www/wwwroot/mcp_deepwiki/output/{safe_name}"
        
        # 1. Fetch from DeepWiki MCP if data is missing
        if not os.path.exists(base_dir) or not glob.glob(os.path.join(base_dir, "*Overview.md")):
            logger.info(f"ğŸ“¥ æ•°æ®ç¼ºå¤±ï¼Œä» DeepWiki MCP è·å–: {repo_name}")
            try:
                await mcp_client.fetch_and_save(repo_name)
            except Exception as e:
                # If MCP fails, it's likely unindexed or a connection issue. Skip for now to avoid dead loops.
                raise Exception(f"MCP fetch failed: {e}. The repository might not be indexed in DeepWiki.")

        if not os.path.exists(base_dir):
            raise Exception(f"Repo data folder not found after MCP fetch: {base_dir}")

        # Find Overview file
        overview_files = glob.glob(os.path.join(base_dir, "*Overview.md"))

        # Check if it's a cold repository (only has 1 document file)
        all_md_files = glob.glob(os.path.join(base_dir, "*.md"))
        if len(all_md_files) <= 1:
            logger.warning(f"âš ï¸ å†·é—¨ä»“åº“æ£€æµ‹ï¼š{repo_name} åªæœ‰ {len(all_md_files)} ä¸ªæ–‡æ¡£ï¼Œæ ‡è®°ä¸ºè·³è¿‡")
            db_repo.status = ProcessingStatus.SKIPPED
            db_repo.error_message = f"å†·é—¨ä»“åº“ï¼šä»…æœ‰ {len(all_md_files)} ä¸ªæ–‡æ¡£ï¼ˆéœ€è¦ Overview.mdï¼‰"
            db.commit()
            return

        if not overview_files:
            raise Exception("No Overview.md found")
        target_file = overview_files[0]

        with open(target_file, "r", encoding="utf-8") as f:
            content = f.read()

        # Fetch README from GitHub
        logger.info(f"ğŸ“– æ­£åœ¨æ‹‰å– GitHub README...")
        readme_content = await github_monitor.fetch_repo_readme(repo_name)
        if readme_content:
            logger.info(f"âœ… æˆåŠŸè·å– README ({len(readme_content)} å­—ç¬¦)")
        else:
            logger.info(f"â„¹ï¸ æœªæ‰¾åˆ° READMEï¼Œç»§ç»­å¤„ç†")

        # 2. RAG Refine
        logger.info(f"âœï¸ æ­£åœ¨ç”Ÿæˆåˆç¨¿...")
        # Generate Draft with README
        draft = await rag_refiner.generate_draft(content, readme_content)

        # Generate AI Title
        logger.info(f"ğŸ·ï¸ æ­£åœ¨ç”Ÿæˆæ–‡æ¡£æ ‡é¢˜...")
        ai_title = await rag_refiner.generate_title(
            repo_name=repo_name,
            description=repo_data.get("description") or "",
            overview_content=content[:500]
        )
        
        # Select documents
        logger.info(f"ğŸ“š æ­£åœ¨é€‰æ‹©ç›¸å…³æ–‡æ¡£...")
        candidate_files = [os.path.basename(p) for p in glob.glob(os.path.join(base_dir, "*.md")) if os.path.abspath(p) != os.path.abspath(target_file)]
        selected_files = await rag_refiner.select_documents_for_rag(draft, candidate_files)

        # Build Knowledge Base
        logger.info(f"ğŸ§  æ­£åœ¨æ„å»ºå‘é‡çŸ¥è¯†åº“...")
        await rag_refiner.build_knowledge_base(base_dir, target_file, selected_files)

        # Final Expand
        logger.info(f"ğŸ”„ æ­£åœ¨é€šè¿‡ RAG æ‰©å±•å†…å®¹...")
        final_content = await rag_refiner.expand_with_rag(draft)
        
        # 3. Upload to Feishu
        logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ åˆ°é£ä¹¦çŸ¥è¯†åº“...")
        # Use AI-generated title with repo name
        title = f"{repo_name} - {ai_title}"
        logger.info(f"ğŸ“Œ æ–‡æ¡£æ ‡é¢˜ï¼š{title}")

        if not db_repo.feishu_doc_token:
            logger.info(f"ğŸ†• åˆ›å»ºæ–°çš„é£ä¹¦æ–‡æ¡£èŠ‚ç‚¹")
            doc_token = await feishu_service.create_node(title=title)
            if doc_token:
                db_repo.feishu_doc_token = doc_token
                db.commit()
        else:
            logger.info(f"ğŸ“ æ›´æ–°å·²æœ‰é£ä¹¦æ–‡æ¡£")
            doc_token = db_repo.feishu_doc_token

        if doc_token:
            # Note: update_document_content currently appends content.
            # In a production scenario, you might want to clear existing blocks first.
            await feishu_service.update_document_content(doc_token, final_content)

            # 4. Notify
            logger.info(f"ğŸ”” å‘é€é€šçŸ¥...")
            await feishu_service.send_card_notification(
                title=f"RAG Refined Wiki: {repo_name}",
                summary=repo_data.get("description") or "Documentation optimized via RAG workflow.",
                url=f"https://feishu.cn/docx/{doc_token}"
            )
            # Add plain text webhook notification
            await feishu_service.send_webhook_notification(repo_name, doc_token)
        
        logger.info(f"âœ… ä»“åº“å¤„ç†å®Œæˆ: {repo_name}")
        db_repo.status = ProcessingStatus.COMPLETED
    except Exception as e:
        error_msg = str(e)
        # Check if it's a cold repository error
        is_cold_repo = (
            "No Overview.md found" in error_msg or
            ("MCP fetch failed" in error_msg and "unindexed" in error_msg.lower()) or
            ("TaskGroup" in error_msg and "sub-exception" in error_msg)
        )

        if is_cold_repo:
            logger.warning(f"âš ï¸ å†·é—¨ä»“åº“ [{repo_name}]: {error_msg}")
            db_repo.status = ProcessingStatus.SKIPPED
            db_repo.error_message = f"å†·é—¨ä»“åº“ï¼š{error_msg}"
        else:
            logger.error(f"âŒ å¤„ç†å¤±è´¥ [{repo_name}]: {error_msg}")
            db_repo.status = ProcessingStatus.FAILED
            db_repo.error_message = error_msg
    
    db.commit()

async def queue_worker():
    """é˜Ÿåˆ—å¤„ç†å·¥ä½œå™¨ - æŒ‰é¡ºåºå¤„ç†é˜Ÿåˆ—ä¸­çš„å•ä¸ªä»“åº“ä»»åŠ¡"""
    logger.info("ğŸ”„ é˜Ÿåˆ—å·¥ä½œå™¨å·²å¯åŠ¨")

    while True:
        try:
            # ç­‰å¾…ä¸‹ä¸€ä¸ªä»»åŠ¡
            task = await task_queue.get_next_task()
            task_queue.is_processing = True
            task_queue.current_task = task

            task_type = task["type"]
            repo_data = task["data"]
            repo_name = repo_data.get("full_name") or repo_data.get("repo_name", "Unknown")

            logger.info(f"ğŸ“‹ å¼€å§‹å¤„ç†ä»“åº“: {repo_name} (ä»»åŠ¡ç±»å‹: {task_type.value})")

            try:
                if task_type == TaskType.SYNC:
                    # å¤„ç†å•ä¸ªä»“åº“çš„åŒæ­¥ä»»åŠ¡
                    db = SessionLocal()
                    try:
                        await process_repo_workflow(db, repo_data)
                    finally:
                        db.close()

                elif task_type == TaskType.REGENERATE:
                    # å¤„ç†é‡æ–°ç”Ÿæˆä»»åŠ¡
                    db = SessionLocal()
                    try:
                        await regenerate_repo_workflow_impl(db, repo_data["repo_id"])
                    finally:
                        db.close()

                logger.info(f"âœ… ä»“åº“å¤„ç†å®Œæˆ: {repo_name}")

            except Exception as e:
                logger.error(f"âŒ ä»“åº“å¤„ç†å¤±è´¥ [{repo_name}]: {e}")

            finally:
                task_queue.is_processing = False
                task_queue.mark_task_done()  # ä½¿ç”¨æ–°æ–¹æ³•æ ‡è®°å®Œæˆ

        except Exception as e:
            logger.error(f"âŒ é˜Ÿåˆ—å·¥ä½œå™¨é”™è¯¯: {e}")
            await asyncio.sleep(1)  # é¿å…å¿«é€Ÿå¾ªç¯

async def sync_task_impl(sync_all: bool = False, silent: bool = False):
    """åŒæ­¥ä»»åŠ¡çš„å®é™…å®ç° - ä¸å†æŸ¥è¯¢æ•°æ®åº“ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ä»“åº“åˆ—è¡¨"""
    # æ³¨æ„ï¼šä»“åº“åˆ—è¡¨å·²ç»åœ¨ sync_task ä¸­æŸ¥è¯¢å¹¶ä¼ å…¥
    # è¿™é‡Œåªæ˜¯ä¸ºäº†å…¼å®¹æ—§çš„è°ƒç”¨æ–¹å¼ï¼Œå¦‚æœ sync_task ç›´æ¥è°ƒç”¨ï¼ˆä¸å¸¦ä»“åº“åˆ—è¡¨ï¼‰
    # åˆ™éœ€è¦åœ¨è¿™é‡ŒæŸ¥è¯¢

    if not silent:
        logger.info(f"ğŸ”„ å¼€å§‹åŒæ­¥ä»»åŠ¡ (sync_all={sync_all})")

    db = SessionLocal()
    try:
        # Fetch new stars and pending repos (for backward compatibility)
        logger.info(f"â­ æ­£åœ¨è·å– GitHub æœ€æ–° star...")
        stars = await github_monitor.fetch_recent_stars(limit=10)
        logger.info(f"ğŸ“¦ å‘ç° {len(stars)} ä¸ªæ–°çš„ star ä»“åº“")
        for star in stars:
            await process_repo_workflow(db, star)

        # Process pending/failed repositories from database (only FAILED and PENDING, not SKIPPED)
        pending_repos = db.query(ProcessedRepo).filter(
            (ProcessedRepo.status == ProcessingStatus.PENDING) |
            (ProcessedRepo.status == ProcessingStatus.FAILED)
        ).all()

        if pending_repos:
            if not silent:
                logger.info(f"ğŸ“‹ å‘ç° {len(pending_repos)} ä¸ªå¾…å¤„ç†/å¤±è´¥çš„å†å²ä»“åº“")
        elif not silent:
            logger.info(f"âœ¨ æ²¡æœ‰å¾…å¤„ç†çš„å†å²ä»“åº“")

        for repo in pending_repos:
            # Convert db record to dict format expected by process_repo_workflow
            repo_data = {
                "id": repo.repo_id,
                "full_name": repo.repo_name,
                "html_url": repo.repo_url,
                "description": repo.description
            }
            await process_repo_workflow(db, repo_data)
    finally:
        db.close()
        if not silent:
            logger.info("âœ… åŒæ­¥ä»»åŠ¡å®Œæˆ")

async def regenerate_repo_workflow_impl(db: Session, repo_id: str):
    """é‡æ–°ç”Ÿæˆä»»åŠ¡çš„å®é™…å®ç°"""
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    if not db_repo:
        logger.error(f"âŒ ä»“åº“ä¸å­˜åœ¨: {repo_id}")
        return

    repo_name = db_repo.repo_name
    logger.info(f"ğŸ”„ é‡æ–°ç”Ÿæˆæ–‡æ¡£: {repo_name}")

    # Mark as processing
    db_repo.status = ProcessingStatus.PROCESSING
    db_repo.error_message = None
    db.commit()

    try:
        # Prepare repo data
        repo_data = {
            "id": db_repo.repo_id,
            "full_name": db_repo.repo_name,
            "html_url": db_repo.repo_url,
            "description": db_repo.description
        }

        # Run the full workflow
        await process_repo_workflow(db, repo_data)

        # If we have a doc_token and completed, clear the old content and regenerate
        if db_repo.feishu_doc_token and db_repo.status == ProcessingStatus.COMPLETED:
            logger.info(f"ğŸ—‘ï¸ æ¸…ç©ºæ—§æ–‡æ¡£å†…å®¹...")
            await feishu_service.clear_document_content(db_repo.feishu_doc_token)

            # Re-run the RAG and upload part
            safe_name = repo_name.replace("/", "_")
            base_dir = f"/www/wwwroot/mcp_deepwiki/output/{safe_name}"

            overview_files = glob.glob(os.path.join(base_dir, "*Overview.md"))
            if not overview_files:
                raise Exception("No Overview.md found")

            target_file = overview_files[0]
            with open(target_file, "r", encoding="utf-8") as f:
                content = f.read()

            # Fetch README
            readme_content = await github_monitor.fetch_repo_readme(repo_name)

            # Generate Draft
            draft = await rag_refiner.generate_draft(content, readme_content)

            # Select documents
            candidate_files = [os.path.basename(p) for p in glob.glob(os.path.join(base_dir, "*.md")) if os.path.abspath(p) != os.path.abspath(target_file)]
            selected_files = await rag_refiner.select_documents_for_rag(draft, candidate_files)

            # Build Knowledge Base
            await rag_refiner.build_knowledge_base(base_dir, target_file, selected_files)

            # Final Expand
            final_content = await rag_refiner.expand_with_rag(draft)

            # Generate AI Title
            ai_title = await rag_refiner.generate_title(
                repo_name=repo_name,
                description=repo_data.get("description") or "",
                overview_content=content[:500]
            )

            # Upload to Feishu (will append to empty doc)
            title = f"{repo_name} - {ai_title}"
            logger.info(f"ğŸ“Œ é‡æ–°ç”Ÿæˆæ–‡æ¡£æ ‡é¢˜ï¼š{title}")

            await feishu_service.update_document_content(db_repo.feishu_doc_token, final_content)

            # Send notification
            await feishu_service.send_card_notification(
                title=f"ğŸ”„ æ–‡æ¡£é‡æ–°ç”Ÿæˆ: {repo_name}",
                summary=repo_data.get("description") or "Documentation has been regenerated.",
                url=f"https://feishu.cn/docx/{db_repo.feishu_doc_token}"
            )
            await feishu_service.send_webhook_notification(repo_name, db_repo.feishu_doc_token)

            logger.info(f"âœ… é‡æ–°ç”Ÿæˆå®Œæˆ: {repo_name}")

    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥ [{repo_name}]: {error_msg}")

        # Check if it's a cold repository error
        is_cold_repo = (
            "No Overview.md found" in error_msg or
            ("MCP fetch failed" in error_msg and "unindexed" in error_msg.lower()) or
            ("TaskGroup" in error_msg and "sub-exception" in error_msg)
        )

        if is_cold_repo:
            db_repo.status = ProcessingStatus.SKIPPED
            db_repo.error_message = f"å†·é—¨ä»“åº“ï¼š{error_msg}"
        else:
            db_repo.status = ProcessingStatus.FAILED
            db_repo.error_message = error_msg

        db.commit()

async def sync_task(sync_all: bool = False, silent: bool = False):
    """åŒæ­¥ä»»åŠ¡ - å°†å•ä¸ªä»“åº“é€ä¸ªåŠ å…¥é˜Ÿåˆ—"""
    db = SessionLocal()
    try:
        # 1. Fetch new star repositories from GitHub
        stars = await github_monitor.fetch_recent_stars(limit=10)

        # 2. Query pending/failed repositories
        pending_repos = db.query(ProcessedRepo).filter(
            (ProcessedRepo.status == ProcessingStatus.PENDING) |
            (ProcessedRepo.status == ProcessingStatus.FAILED)
        ).all()

        # Collect all repos to process
        repos_to_process = []

        # Add new stars
        for star in stars:
            repo_id = str(star["id"])
            # Check if already in queue
            if not task_queue.is_repo_in_queue(repo_id):
                repos_to_process.append(star)  # ç›´æ¥ä½¿ç”¨ GitHub API çš„åŸå§‹æ ¼å¼

        # Add pending/failed reposï¼ˆè½¬æ¢ä¸º GitHub API æ ¼å¼ï¼‰
        for repo in pending_repos:
            # Check if already in queue
            if not task_queue.is_repo_in_queue(repo.repo_id):
                repos_to_process.append({
                    "id": repo.repo_id,  # ä½¿ç”¨ "id" è€Œä¸æ˜¯ "repo_id"
                    "full_name": repo.repo_name,  # ä½¿ç”¨ "full_name" è€Œä¸æ˜¯ "repo_name"
                    "html_url": repo.repo_url,  # ä½¿ç”¨ "html_url" è€Œä¸æ˜¯ "repo_url"
                    "description": repo.description,
                    "type": "retry"
                })

        # Add each repo as individual task to queue
        added_count = 0
        for repo_data in repos_to_process:
            await task_queue.add_repo_task(TaskType.SYNC, repo_data)
            added_count += 1

        if added_count > 0 and not silent:
            logger.info(f"ğŸ“¥ å·²åŠ å…¥ {added_count} ä¸ªä»“åº“åˆ°é˜Ÿåˆ—")

    finally:
        db.close()

async def regenerate_repo_workflow(db: Session, repo_id: str):
    """é‡æ–°ç”Ÿæˆä»»åŠ¡ - å°†å•ä¸ªä»“åº“åŠ å…¥é˜Ÿåˆ—"""
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    if not db_repo:
        logger.error(f"âŒ ä»“åº“ä¸å­˜åœ¨: {repo_id}")
        return

    # æ£€æŸ¥æ˜¯å¦å·²åœ¨é˜Ÿåˆ—ä¸­
    if task_queue.is_repo_in_queue(repo_id):
        logger.info(f"â„¹ï¸ ä»“åº“å·²åœ¨é˜Ÿåˆ—ä¸­: {db_repo.repo_name}")
        return

    # æ·»åŠ å•ä¸ªä»“åº“ä»»åŠ¡
    await task_queue.add_repo_task(TaskType.REGENERATE, {
        "repo_id": repo_id,  # REGENERATE ä»»åŠ¡ç‰¹æ®Šï¼Œéœ€è¦ "repo_id" å­—æ®µ
        "full_name": db_repo.repo_name,  # ç»Ÿä¸€ä½¿ç”¨ "full_name"
        "html_url": db_repo.repo_url,  # ç»Ÿä¸€ä½¿ç”¨ "html_url"
        "description": db_repo.description,
        "type": "regenerate"
    })

    logger.info(f"ğŸ“¥ é‡æ–°ç”Ÿæˆä»»åŠ¡å·²åŠ å…¥é˜Ÿåˆ—: {db_repo.repo_name}")

# Background Scheduler
async def scheduler_loop():
    while True:
        await asyncio.sleep(60)  # Run every 60 seconds (1 minute)
        await sync_task(sync_all=False, silent=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global github_monitor, feishu_service, rag_refiner, mcp_client
    logger.info("=" * 50)
    logger.info("ğŸš€ MCP DeepWiki æœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("=" * 50)

    github_monitor = GitHubMonitor(os.getenv("GITHUB_TOKEN"))
    logger.info("âœ… GitHub ç›‘æ§å™¨å·²åˆå§‹åŒ–")

    feishu_service = FeishuService(
        os.getenv("FEISHU_APP_ID"),
        os.getenv("FEISHU_APP_SECRET"),
        os.getenv("FEISHU_SPACE_ID"),
        os.getenv("FEISHU_WEBHOOK_URL")
    )
    logger.info("âœ… é£ä¹¦æœåŠ¡å·²åˆå§‹åŒ–")

    rag_refiner = RAGRefiner()
    logger.info("âœ… RAG ç²¾ç‚¼å™¨å·²åˆå§‹åŒ–")

    mcp_client = DeepWikiMCPClient()
    logger.info("âœ… DeepWiki MCP å®¢æˆ·ç«¯å·²åˆå§‹åŒ–")

    # å¯åŠ¨é˜Ÿåˆ—å·¥ä½œå™¨
    logger.info("ğŸ”„ å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—å·¥ä½œå™¨...")
    asyncio.create_task(queue_worker())

    logger.info("â° å¯åŠ¨åå°è°ƒåº¦å™¨ (æ¯60ç§’æ‰§è¡Œä¸€æ¬¡)")
    asyncio.create_task(scheduler_loop())

    # Initialize DB with historical stars on first run
    db = SessionLocal()
    try:
        if db.query(ProcessedRepo).count() == 0:
            logger.info("ğŸ¯ é¦–æ¬¡è¿è¡Œï¼šæ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“ï¼Œå¯¼å…¥æ‰€æœ‰å†å² star ä»“åº“...")
            stars = await github_monitor.fetch_all_stars()
            logger.info(f"ğŸ“Š å…±æ‰¾åˆ° {len(stars)} ä¸ª star ä»“åº“")
            for star in stars:
                repo_id = str(star["id"])
                if not db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first():
                    # Check if we already have the output folder for this repo
                    safe_name = star["full_name"].replace("/", "_")
                    base_dir = f"/www/wwwroot/mcp_deepwiki/output/{safe_name}"

                    status = ProcessingStatus.PENDING
                    # If refined file already exists, mark as completed
                    if os.path.exists(os.path.join(base_dir, "refined", "02_Overview_Refined.md")):
                        status = ProcessingStatus.COMPLETED

                    repo = ProcessedRepo(
                        repo_id=repo_id,
                        repo_name=star["full_name"],
                        repo_url=star["html_url"],
                        description=star.get("description"),
                        status=status
                    )
                    db.add(repo)
            db.commit()
            logger.info(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(stars)} ä¸ªä»“åº“")
        else:
            logger.info("âœ… æ•°æ®åº“å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é¦–æ¬¡è¿è¡Œè®¾ç½®")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    finally:
        db.close()

    logger.info("=" * 50)
    logger.info("ğŸ‰ MCP DeepWiki æœåŠ¡å¯åŠ¨å®Œæˆï¼")
    logger.info("=" * 50)
    yield

app = FastAPI(lifespan=lifespan)

@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/api/repos")
async def get_repos(db: Session = Depends(get_db)):
    return db.query(ProcessedRepo).order_by(ProcessedRepo.updated_at.desc()).all()

@app.post("/api/retry/{repo_id}")
async def retry_repo(repo_id: str, db: Session = Depends(get_db)):
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    if not db_repo:
        logger.warning(f"âš ï¸ é‡è¯•å¤±è´¥ï¼šä»“åº“ {repo_id} ä¸å­˜åœ¨")
        return {"error": "Repository not found"}, 404

    logger.info(f"ğŸ”„ æ­£åœ¨é‡è¯•ä»“åº“: {db_repo.repo_name} (å½“å‰çŠ¶æ€: {db_repo.status.value})")
    # Reset status to PENDING to allow it to be picked up (works for FAILED and SKIPPED)
    db_repo.status = ProcessingStatus.PENDING
    db_repo.error_message = None
    db_repo.updated_at = datetime.datetime.now(datetime.UTC)
    db.commit()

    # Trigger a sync task in queue to process immediately
    await sync_task(False, True)

    logger.info(f"âœ… å·²å°†ä»“åº“ {db_repo.repo_name} æ ‡è®°ä¸ºå¾…å¤„ç†å¹¶åŠ å…¥é˜Ÿåˆ—")
    return {"status": "retrying"}

@app.post("/api/regenerate/{repo_id}")
async def regenerate_repo(repo_id: str, db: Session = Depends(get_db)):
    """Regenerate documentation for a completed repository"""
    db_repo = db.query(ProcessedRepo).filter(ProcessedRepo.repo_id == repo_id).first()
    if not db_repo:
        logger.warning(f"âš ï¸ é‡æ–°ç”Ÿæˆå¤±è´¥ï¼šä»“åº“ {repo_id} ä¸å­˜åœ¨")
        return {"error": "Repository not found"}, 404

    if not db_repo.feishu_doc_token:
        logger.warning(f"âš ï¸ ä»“åº“ {db_repo.repo_name} å°šæœªç”Ÿæˆé£ä¹¦æ–‡æ¡£ï¼Œæ— æ³•é‡æ–°ç”Ÿæˆ")
        return {"error": "No Feishu document found"}, 400

    logger.info(f"ğŸ”„ æ­£åœ¨é‡æ–°ç”Ÿæˆæ–‡æ¡£: {db_repo.repo_name}")
    # Add to queue instead of executing immediately
    await regenerate_repo_workflow(db, repo_id)

    return {"status": "regenerating"}

@app.post("/trigger")
async def trigger(sync_all: bool = False):
    logger.info(f"ğŸ¯ æ‰‹åŠ¨è§¦å‘åŒæ­¥ä»»åŠ¡ (sync_all={sync_all})")
    await sync_task(sync_all)
    return {"status": "triggered"}

# Add API endpoint to check queue status
@app.get("/api/queue/status")
async def get_queue_status():
    """è·å–å½“å‰é˜Ÿåˆ—çŠ¶æ€ - è¿”å›å•ä¸ªä»“åº“åˆ—è¡¨"""
    # Get waiting tasks from queue using the new method
    waiting_tasks = task_queue.get_waiting_tasks()

    # Build response - å•ä¸ªä»“åº“åˆ—è¡¨
    waiting_repos = []
    for task in waiting_tasks:
        data = task["data"]
        waiting_repos.append({
            "type": task["type"].value,
            "added_at": task["added_at"].isoformat(),
            "repo_id": data.get("repo_id") or str(data.get("id", "")),  # å…¼å®¹ä¸¤ç§æ ¼å¼
            "repo_name": data.get("full_name") or data.get("repo_name", "Unknown"),
            "repo_type": data.get("type", "sync")
        })

    # Current task repo info
    current_repo = None
    if task_queue.current_task:
        data = task_queue.current_task["data"]
        current_repo = {
            "type": task_queue.current_task["type"].value,
            "added_at": task_queue.current_task["added_at"].isoformat(),
            "repo_id": data.get("repo_id") or str(data.get("id", "")),  # å…¼å®¹ä¸¤ç§æ ¼å¼
            "repo_name": data.get("full_name") or data.get("repo_name", "Unknown"),
            "repo_type": data.get("type", "sync")
        }

    return {
        "queue_size": len(waiting_repos),
        "is_processing": task_queue.is_processing,
        "current_repo": current_repo,
        "waiting_repos": waiting_repos
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8002)
